{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regression \n",
    "\n",
    "## Machine Learning and Adaptive Intelligence\n",
    "\n",
    "### Mauricio Álvarez \n",
    "\n",
    "### Based on slides by Neil D. Lawrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pods\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Review\n",
    "- Last time: Looked at objective functions for movie recommendation.\n",
    "- Minimized sum of squares objective by steepest descent and stochastic gradients.\n",
    "- This time: explore least squares for regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regression Examples\n",
    "\n",
    "-   Predict a real value, $y_i$ given some inputs\n",
    "    $x_i$.\n",
    "\n",
    "-   Predict quality of meat given spectral measurements (Tecator data).\n",
    "\n",
    "-   Radiocarbon dating, the C14 calibration curve: predict age given\n",
    "    quantity of C14 isotope.\n",
    "\n",
    "-   Predict quality of different Go or Backgammon moves given expert\n",
    "    rated training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Olympic 100m Data\n",
    "\n",
    "-  Gold medal times for Olympic 100 m runners since 1896.\n",
    "\n",
    "![image](./diagrams/100m_final_start.jpg)\n",
    "Image from Wikimedia Commons <http://bit.ly/191adDC>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Olympic 100m Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"diagrams/male100.jpeg\" width=\"500\" height=\"40\" align=center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Olympic Marathon Data\n",
    "\n",
    "-   Gold medal times for Olympic Marathon since 1896.\n",
    "\n",
    "-   Marathons before 1924 didn’t have a standardised distance.\n",
    "\n",
    "-   Present results using pace per km.\n",
    "\n",
    "-   In 1904 Marathon was badly organised leading to very slow times.\n",
    "\n",
    "<img src=\"diagrams/Eliud_Kipchoge.jpg\" width=\"300\" height=\"40\" align=center>\n",
    "\n",
    "Image from Wikimedia Commons [Eliud Kipchoge](https://commons.wikimedia.org/wiki/File:Eliud_Kipchoge_in_Berlin_-_2015_(cropped).jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Olympic Marathon Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acquiring resource: olympic_marathon_men\n",
      "\n",
      "Details of data: \n",
      "Olympic mens' marathon gold medal winning times from 1896 to 2012. Time given in pace (minutes per kilometer). Data is originally downloaded and collated from Wikipedia, we are not responsible for errors in the data\n",
      "\n",
      "After downloading the data will take up 584 bytes of space.\n",
      "\n",
      "Data will be stored in /home/vamsi-dx/ods_data_cache/olympic_marathon_men.\n",
      "\n",
      "Do you wish to proceed with the download? [yes/no]\n",
      "yes\n",
      "Downloading  http://staffwww.dcs.shef.ac.uk/people/N.Lawrence/dataset_mirror/olympic_marathon_men/olympicMarathonTimes.csv -> /home/vamsi-dx/ods_data_cache/olympic_marathon_men/olympicMarathonTimes.csv\n",
      "[==============================]   0.001/0.001MB                                                \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f9ec688dbe0>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa8AAAGfCAYAAADoEV2sAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAMIhvAAA4RBeAIBwCC8AQDiEFwAgnP8PG2GxWNvjk4UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9ec688dfd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pods.datasets.olympic_marathon_men()\n",
    "# Adding 2016 time\n",
    "np.append(data['X'], 2016)\n",
    "np.append(data['Y'], (2*60+8+44/60)/42.195)\n",
    "f, ax = plt.subplots(figsize=(7,7))\n",
    "ax.plot(data['X'], data['Y'], 'ro',markersize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is Machine Learning?\n",
    "\n",
    "$$ \\text{data} + \\text{model} = \\text{prediction}$$\n",
    "\n",
    "-   $\\text{data}$ : observations, could be actively or passively\n",
    "    acquired (meta-data).\n",
    "\n",
    "-   $\\text{model}$ : assumptions, based on previous experience (other data!\n",
    "    transfer learning etc), or beliefs about the regularities of\n",
    "    the universe. Inductive bias.\n",
    "\n",
    "-   $\\text{prediction}$ : an action to be taken or a categorization or a\n",
    "    quality score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regression: Linear Releationship\n",
    "\n",
    "$$y_i = m x_i + c$$\n",
    "\n",
    "-   $y_i$ : winning time/pace.\n",
    "\n",
    "-   $x_i$ : year of Olympics.\n",
    "\n",
    "-   $m$ : rate of improvement over time.\n",
    "\n",
    "-   $c$ : winning time at year 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Overdetermined System\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"diagrams/two_points.jpeg\" width=\"500\" height=\"40\" align=center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# $y = mx + c$\n",
    "\n",
    "point 1: $x = 1$, $y=3$ $$3 = m + c$$ \n",
    "point 2: $x = 3$, $y=1$ $$1 = 3m + c$$ \n",
    "\n",
    "*We know how to solve this system of two equations and two unknowns*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"diagrams/two_points_plus_line.jpeg\" width=\"500\" height=\"40\" align=center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## We now observe a new point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"diagrams/three_points.jpeg\" width=\"500\" height=\"40\" align=center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# $y = mx + c$\n",
    "\n",
    "point 1: $x = 1$, $y=3$ $$3 = m + c$$ \n",
    "point 2: $x = 3$, $y=1$ $$1 = 3m + c$$ \n",
    "point 3: $x = 2$, $y=2.5$ $$2.5 = 2m + c$$\n",
    "\n",
    "*Overdetermined system (more equations than unknowns): three equations and two unknowns* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"diagrams/Pierre-Simon_Laplace.png\" align=center width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# $y = mx + c + \\epsilon$\n",
    "\n",
    "point 1: $x = 1$, $y=3$ \n",
    "$$3 = m + c + \\epsilon_1$$ \n",
    "\n",
    "point 2: $x = 3$, $y=1$ \n",
    "$$1 = 3m + c + \\epsilon_2$$ \n",
    "\n",
    "point 3: $x = 2$, $y=2.5$ \n",
    "$$2.5 = 2m + c + \\epsilon_3$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Gaussian Density\n",
    "\n",
    "Perhaps the most common probability density.\n",
    "\n",
    "\\begin{align*}\n",
    "p(y| \\mu, \\sigma^2) & = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{(y - \\mu)^2}{2\\sigma^2}\\right)\\\\\n",
    "                    & \\buildrel\\triangle\\over = \\mathcal{N}(y|\\mu, \\sigma^2)\n",
    "\\end{align*}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gaussian Density\n",
    "\n",
    "![](./diagrams/gaussian_of_height.svg)\n",
    "\n",
    "The Gaussian PDF with $\\mu=1.7$ and variance $\\sigma^2=\n",
    "  0.0225$. Mean shown as red line. It could represent the heights of a population of\n",
    "  students."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gaussian Density\n",
    "$$\n",
    "\\mathcal{N}(y|\\mu, \\sigma^2) =  \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y-\\mu)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "$\\sigma^2$ is the variance of the density and $\\mu$ is the mean.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Two Important Gaussian Properties\n",
    "\n",
    "**Sum of Gaussian**\n",
    "\n",
    "-   Sum of Gaussian variables is also Gaussian.\n",
    "    \n",
    "    $$y_i \\sim \\mathcal{N}(\\mu, \\sigma^2)$$ \n",
    "    \n",
    "    And the sum is distributed as\n",
    "    \n",
    "    $$\\sum_{i=1}^{n} y_i \\sim \\mathcal{N}\\left(\\sum_{i=1}^n \\mu_i,\\sum_{i=1}^n \\sigma_i^2\\right)$$\n",
    "    \n",
    "    (*Aside*: As sum increases, sum of non-Gaussian, finite variance variables is\n",
    "    also Gaussian [central limit theorem](https://en.wikipedia.org/wiki/Central_limit_theorem).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Two Important Gaussian Properties\n",
    "\n",
    "**Scaling a Gaussian**\n",
    "\n",
    "-   Scaling a Gaussian leads to a Gaussian.\n",
    "    \n",
    "    $$y \\sim \\mathcal{N}(\\mu, \\sigma^2)$$\n",
    "    \n",
    "    And the scaled density is distributed as\n",
    "    \n",
    "    $$w y \\sim \\mathcal{N}(w\\mu,w^2 \\sigma^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Laplace's Idea\n",
    "\n",
    "### A Probabilistic Process\n",
    "\n",
    "-   Set the mean of Gaussian to be a function.\n",
    "    \n",
    "    $$p\\left(y_i|x_i\\right)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp \\left(-\\frac{\\left(y_i-f\\left(x_i\\right)\\right)^{2}}\n",
    "    {2\\sigma^2}\\right).$$\n",
    "\n",
    "\n",
    "-   This gives us a ‘noisy function’.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Height as a Function of Weight\n",
    "\n",
    "-   In the standard Gaussian, parametized by mean and variance.\n",
    "\n",
    "-   Make the mean a linear function of an *input*.\n",
    "\n",
    "-   This leads to a regression model. \n",
    "\n",
    "    \\begin{align*}\n",
    "               y_i=    &  f\\left(x_i\\right)+\\epsilon_i,\\\\\n",
    "       \\epsilon_i \\sim &  \\mathcal{N}(0, \\sigma^2).\n",
    "     \\end{align*}\n",
    "        \n",
    "-   Assume $y_i$ is height and $x_i$ is weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data Point Likelihood\n",
    "\n",
    "-   Likelihood of an individual data point\n",
    "    $$p\\left(y_i|x_i,m,c\\right)=\\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp \\left(-\\frac{\\left(y_i-mx_i-c\\right)^{2}}{2\\sigma^2}\\right).$$\n",
    "    \n",
    "\n",
    "-   Parameters are gradient, $m$, offset, $c$ of the function and noise\n",
    "    variance $\\sigma^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data Set Likelihood\n",
    "\n",
    "-   If the noise, $\\epsilon_i$ is sampled independently for each\n",
    "    data point.\n",
    "\n",
    "-   Each data point is independent (given $m$ and $c$).\n",
    "\n",
    "-   For independent variables:\n",
    "    $$p(\\mathbf{y}) = \\prod_{i=1}^n p(y_i)$$\n",
    "    $$p(\\mathbf{y}|\\mathbf{x}, m, c) = \\prod_{i=1}^n p(y_i|x_i, m, c)$$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### For Gaussian \n",
    "\n",
    "- i.i.d. assumption\n",
    "    \n",
    "    $$p(\\mathbf{y}|\\mathbf{x}, m, c) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp \\left(-\\frac{\\left(y_i-mx_i-  \n",
    "            c\\right)^{2}}{2\\sigma^2}\\right).$$\n",
    "            \n",
    "    $$p(\\mathbf{y}|\\mathbf{x}, m, c) = \\frac{1}{\\left(2\\pi \\sigma^2\\right)^{\\frac{n}{2}}}\\exp \\left(-  \n",
    "            \\frac{\\sum_{i=1}^n\\left(y_i-mx_i-c\\right)^{2}}{2\\sigma^2}\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Log Likelihood Function\n",
    "\n",
    "-   Normally work with the log likelihood:\n",
    "    \n",
    "    $$L(m,c,\\sigma^{2})=-\\frac{n}{2}\\log 2\\pi -\\frac{n}{2}\\log \\sigma^2 -\\sum _{i=1}^{n}\\frac{\\left(y_i-mx_i-    \n",
    "        c\\right)^{2}}{2\\sigma^2}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Consistency of Maximum Likelihood\n",
    "\n",
    "\n",
    "-   If data was really generated according to probability we specified.\n",
    "\n",
    "\n",
    "-   Correct parameters will be recovered in limit as\n",
    "    $n \\rightarrow \\infty$.\n",
    "\n",
    "\n",
    "-   This can be proven through sample based approximations (law of large numbers) of “KL divergences”.\n",
    "\n",
    "\n",
    "-   Mainstay of classical statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Probabilistic Interpretation of the Error Function\n",
    "\n",
    "-   Probabilistic Interpretation for Error Function is Negative Log Likelihood.\n",
    "\n",
    "\n",
    "-   *Minimizing* error function is equivalent to *maximizing* log likelihood.\n",
    "\n",
    "\n",
    "-   Maximizing *log likelihood* is equivalent to maximizing the *likelihood* because $\\log$ is monotonic.\n",
    "\n",
    "\n",
    "-   Probabilistic interpretation: Minimizing error function is equivalent to maximum likelihood with respect to   parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Error Function\n",
    "\n",
    "-   Negative log likelihood is the error function leading to an error function\n",
    "\n",
    "    $$E(m,c,\\sigma^{2})=\\frac{n}{2}\\log \\sigma^2 +\\frac{1}{2\\sigma^2}\\sum _{i=1}^{n}\\left(y_i-mx_i-c\\right)^{2}.$$\n",
    "\n",
    "-   Learning proceeds by minimizing this error function for the data\n",
    "    set provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Connection: Sum of Squares Error\n",
    "\n",
    "-   Ignoring terms which don’t depend on $m$ and $c$ gives\n",
    "    \n",
    "    $$E(m, c) \\propto \\sum_{i=1}^n (y_i - f(x_i))^2$$\n",
    "    \n",
    "    where $f(x_i) = mx_i + c$.\n",
    "\n",
    "\n",
    "-   This is known as the *sum of squares* error function.\n",
    "\n",
    "\n",
    "-   Commonly used and is closely associated with the Gaussian likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reminder\n",
    "\n",
    "- Two functions involved:\n",
    "\n",
    "  - Prediction function: $f(x_i)$\n",
    "\n",
    "  - Error, or Objective function: $E(m, c)$\n",
    "\n",
    "\n",
    "- Error function depends on parameters through prediction function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mathematical Interpretation\n",
    "\n",
    "-   What is the mathematical interpretation?\n",
    "\n",
    "    -   There is a cost function.\n",
    "\n",
    "    -   It expresses mismatch between your prediction and reality.\n",
    "        $$E(m, c)=\\sum_{i=1}^n \\left(y_i - mx_i -c\\right)^2$$\n",
    "\n",
    "    -   This is known as the sum of squares error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning is Optimization\n",
    "\n",
    "-   Learning is minimization of the cost function.\n",
    "\n",
    "-   At the minima the gradient is zero.\n",
    "\n",
    "-   Coordinate ascent, find gradient in each coordinate and set to zero.\n",
    "    \\begin{align}\n",
    "     \\frac{\\text{d}E(m)}{\\text{d}m} &= -2\\sum_{i=1}^n x_i\\left(y_i- m x_i - c \\right)\\\\\n",
    "                                   0&= -2\\sum_{i=1}^n x_i\\left(y_i- m x_i - c \\right)\n",
    "    \\end{align}                               \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning is Optimization\n",
    "\n",
    "- Fixed point equations\n",
    "    $$0 =\n",
    "          -2\\sum_{i=1}^n x_iy_i\n",
    "          +2\\sum_{i=1}^n\n",
    "            m x_i^2 +2\\sum_{i=1}^n cx_i$$\n",
    "    $$m  =    \\frac{\\sum_{i=1}^n \\left(y_i\n",
    "          -c\\right)x_i}{\\sum_{i=1}^nx_i^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning is Optimization\n",
    "\n",
    "-   Learning is minimization of the cost function.\n",
    "\n",
    "-   At the minima the gradient is zero.\n",
    "\n",
    "-   Coordinate ascent, find gradient in each coordinate and set to zero.\n",
    "\n",
    "     $$\\frac{\\text{d}E(c)}{\\text{d}c} = -2\\sum_{i=1}^n \\left(y_i- m x_i - c \\right)$$\n",
    "    \n",
    "     $$0 = -2\\sum_{i=1}^n\\left(y_i-m x_i - c \\right)$$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learning is Optimization\n",
    "\n",
    "- Fixed point equations\n",
    "    \\begin{align}\n",
    "        0 &= -2\\sum_{i=1}^n y_i +2\\sum_{i=1}^n m x_i +2n c\\\\\n",
    "        c &= \\frac{\\sum_{i=1}^n \\left(y_i-mx_i\\right)}{n}\n",
    "    \\end{align}    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fixed Point Updates\n",
    "\n",
    "Worked example. \n",
    "   \n",
    "   \\begin{align}\n",
    "       c^{*}=&\\frac{\\sum _{i=1}^{n}\\left(y_i-m^{*}x_i\\right)}{n},\\\\\n",
    "       m^{*}=&\\frac{\\sum _{i=1}^{n}x_i\\left(y_i-c^{*}\\right)}{\\sum _{i=1}^{n}x_i^{2}},\\\\\n",
    "      \\left.\\sigma^2\\right.^{*}=&\\frac{\\sum _{i=1}^{n}\\left(y_i-m^{*}x_i-c^{*}\\right)^{2}}{n}\n",
    "  \\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Important Concepts Not Covered\n",
    "\n",
    "-   Other optimization methods:\n",
    "\n",
    "    -   Second order methods, conjugate gradient, quasi-Newton\n",
    "        and Newton.\n",
    "\n",
    "    -   Effective heuristics such as momentum.\n",
    "\n",
    "-   Local vs global solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reading\n",
    "\n",
    "- Section 1.1-1.2 of Rogers and Girolami (2016) for fitting linear models. \n",
    "- Section 1.2.5 of Bishop (2006) up to equation 1.65.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multi-dimensional Inputs\n",
    "\n",
    "-   Multivariate functions involve more than one input.\n",
    "\n",
    "-   Height might be a function of weight and gender.\n",
    "\n",
    "-   There could be other contributory factors.\n",
    "\n",
    "-   Place these factors in a feature vector $\\mathbf{x}_i$.\n",
    "\n",
    "-   Linear function is now defined as\n",
    "    $$f(\\mathbf{x}_i) = \\sum_{j=1}^p w_j x_{i, j} + c$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vector Notation\n",
    "\n",
    "-   Write in vector notation,\n",
    "    $$f(\\mathbf{x}_i) = \\mathbf{w}^\\top \\mathbf{x}_i + c$$\n",
    "\n",
    "-   Can absorb $c$ into $\\mathbf{w}$ by assuming extra input $x_0$\n",
    "    which is always 1.\n",
    "    $$f(\\mathbf{x}_i) = \\mathbf{w}^\\top \\mathbf{x}_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Log Likelihood for Multivariate Regression\n",
    "\n",
    "-   The likelihood of a single data point is\n",
    "    $$p\\left(y_i|x_i\\right)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\n",
    "        \\left(-\\frac{\\left(y_i-\\mathbf{w}^{\\top}\\mathbf{x}_i\\right)^{2}}{2\\sigma^2}\\right).$$\n",
    "\n",
    "-   Leading to a log likelihood for the data set of\n",
    "    $$L(\\mathbf{w},\\sigma^2)= -\\frac{n}{2}\\log \\sigma^2\n",
    "          -\\frac{n}{2}\\log 2\\pi -\\frac{\\sum\n",
    "            _{i=1}^{n}\\left(y_i-\\mathbf{w}^{\\top}\\mathbf{x}_i\\right)^{2}}{2\\sigma^2}.$$\n",
    "\n",
    "-   And a corresponding error function of\n",
    "    $$E(\\mathbf{w},\\sigma^2)= \\frac{n}{2}\\log\n",
    "          \\sigma^2 + \\frac{\\sum\n",
    "            _{i=1}^{n}\\left(y_i-\\mathbf{w}^{\\top}\\mathbf{x}_i\\right)^{2}}{2\\sigma^2}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Expand the Brackets\n",
    "\n",
    "\\begin{align*}\n",
    "  E(\\mathbf{w},\\sigma^2)  = & \\frac{n}{2}\\log \\sigma^2 + \\frac{1}{2\\sigma^2}\\sum _{i=1}^{n}y_i^{2}-\\frac{1}{\\sigma^2}\\sum _{i=1}^{n}y_i\\mathbf{w}^{\\top}\\mathbf{x}_i\\\\&+\\frac{1}{2\\sigma^2}\\sum _{i=1}^{n}\\mathbf{w}^{\\top}\\mathbf{x}_i\\mathbf{x}_i^{\\top}\\mathbf{w} +\\text{const}.\\\\\n",
    "    = & \\frac{n}{2}\\log \\sigma^2 + \\frac{1}{2\\sigma^2}\\sum _{i=1}^{n}y_i^{2}-\\frac{1}{\\sigma^2}\n",
    "  \\mathbf{w}^\\top\\sum_{i=1}^{n}\\mathbf{x}_iy_i\\\\&+\\frac{1}{2\\sigma^2} \\mathbf{w}^{\\top}\\left[\\sum\n",
    "    _{i=1}^{n}\\mathbf{x}_i\\mathbf{x}_i^{\\top}\\right]\\mathbf{w} +\\text{const}.\n",
    "    \\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multivariate Derivatives\n",
    "\n",
    "-   We will need some multivariate calculus.\n",
    "\n",
    "-   For now some simple multivariate differentiation:\n",
    "    $$\\frac{\\text{d}{\\mathbf{a}^{\\top}}{\\mathbf{w}}}{\\text{d}\\mathbf{w}}=\\mathbf{a}$$\n",
    "    and\n",
    "    $$\\frac{\\mathbf{w}^{\\top}\\mathbf{A}\\mathbf{w}}{\\text{d}\\mathbf{w}}=\\left(\\mathbf{A}+\\mathbf{A}^{\\top}\\right)\\mathbf{w}$$\n",
    "    or if $\\mathbf{A}$ is symmetric (*i.e.*\n",
    "    $\\mathbf{A}=\\mathbf{A}^{\\top}$)\n",
    "    $$\\frac{\\text{d}\\mathbf{w}^{\\top}\\mathbf{A}\\mathbf{w}}{\\text{d}\\mathbf{w}}=2\\mathbf{A}\\mathbf{w}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Differentiate\n",
    "\n",
    "Differentiating with respect to the vector $\\mathbf{w}$ we obtain\n",
    "$$\\frac{\\partial L\\left(\\mathbf{w},\\sigma^2 \\right)}{\\partial \\mathbf{w}}=\\frac{1}{\\sigma^2} \\sum _{i=1}^{n}\\mathbf{x}_iy_i-\\frac{1}{\\sigma^2} \\left[\\sum _{i=1}^{n}\\mathbf{x}_i\\mathbf{x}_i^{\\top}\\right]\\mathbf{w}$$\n",
    "Leading to\n",
    "$$\\mathbf{w}^{*}=\\left[\\sum _{i=1}^{n}\\mathbf{x}_i\\mathbf{x}_i^{\\top}\\right]^{-1}\\sum _{i=1}^{n}\\mathbf{x}_iy_i,$$\n",
    "Rewrite in matrix notation:\n",
    "$$\\sum _{i=1}^{n}\\mathbf{x}_i\\mathbf{x}_i^\\top = \\mathbf{X}^\\top \\mathbf{X}$$\n",
    "$$\\sum _{i=1}^{n}\\mathbf{x}_iy_i = \\mathbf{X}^\\top \\mathbf{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Update Equations\n",
    "\n",
    "-   Update for $\\mathbf{w}^{*}$.\n",
    "    $$\\mathbf{w}^{*} = \\left(\\mathbf{X}^\\top \\mathbf{X}\\right)^{-1} \\mathbf{X}^\\top \\mathbf{y}$$\n",
    "\n",
    "-   The equation for $\\left.\\sigma^2\\right.^{*}$ may also be found\n",
    "    $$\\left.\\sigma^2\\right.^{{*}}=\\frac{\\sum _{i=1}^{n}\\left(y_i-\\left.\\mathbf{w}^{*}\\right.^{\\top}\\mathbf{x}_i\\right)^{2}}{n}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reading\n",
    "\n",
    "- Section 1.3 of Rogers and Girolami (2016) for Matrix & Vector Review.\n",
    " "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
